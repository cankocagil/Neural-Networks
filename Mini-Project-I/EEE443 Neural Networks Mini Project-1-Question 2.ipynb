{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 input Truth Label :\n",
    "input_features = np.array([[0,0,0,0],\n",
    "                           [0,0,0,1],\n",
    "                           [0,0,1,0],\n",
    "                           [0,0,1,1],\n",
    "                           [0,1,0,0],\n",
    "                           [0,1,0,1],\n",
    "                           [0,1,1,0],\n",
    "                           [0,1,1,1],\n",
    "                           [1,0,0,0],\n",
    "                           [1,0,0,1],\n",
    "                           [1,0,1,0],\n",
    "                           [1,0,1,1],\n",
    "                           [1,1,0,0],\n",
    "                           [1,1,0,1],\n",
    "                           [1,1,1,0],\n",
    "                           [1,1,1,1]])\n",
    "\n",
    "# Their correspoding outputs :\n",
    "target_output = np.repeat(1,16).reshape(16,1)\n",
    "target_output[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of Input Array: (16, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Shape of Input Array: {input_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of Output Array: (16, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Shape of Output Array: {target_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Introducing predetermined bias terms :\n",
    "        \n",
    "        # Hidden Layers bias term :\n",
    "        self.bias = np.array([6,3,2,5]).reshape(4,1) \n",
    "        # Output Layers bias term :\n",
    "        self.bias2 = np.array([[1]])\n",
    "        \n",
    "        # Introducing predetermined weight terms :\n",
    "        \n",
    "        # Hidden Layers weight :\n",
    "        self.W1 = np.array([[2,1,4,3],[0,-3,3,3],[-3,3,-2,1],[-2,6,1,-3]]).reshape(4,4)        \n",
    "        # Output Layers weight :\n",
    "        self.W2 = np.array([2,3,4,7]).reshape(4,1)\n",
    "        \n",
    "        # Output of output layer :\n",
    "        self.A2 = None   \n",
    "        \n",
    "        # To track Mean Squared Error function :\n",
    "        self.cost = []\n",
    "        \n",
    "        \n",
    "    def step(self,X):\n",
    "        \"\"\"\n",
    "        Given the arrays, return ; \n",
    "        if {1 , X > 0 \n",
    "           {0 , X <= 0\n",
    "        \"\"\"\n",
    "        return 1 * (X > 0)    \n",
    "        \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray) -> None: \n",
    "        \"\"\"\n",
    "        Given the traning dataset and their labels,\n",
    "        fitting the model, and measure the performance\n",
    "        by validating training dataset.\n",
    "        \"\"\"\n",
    "        # Concatenating hidden layers weights and bias term :\n",
    "        W1 = np.concatenate((self.W1,self.bias),axis = 1)\n",
    "        \n",
    "        # Feed Forwarding :\n",
    "        Z1 = np.dot(X,W1) \n",
    "        \n",
    "        # Output of the activation function : (0/1)\n",
    "        A1 = self.step(Z1)\n",
    "        \n",
    "        # Same procedure for output layer :\n",
    "        W2 = np.concatenate((self.W2,self.bias2))\n",
    "        Z2 = np.dot(A1,W2) \n",
    "        self.A2 = self.step(Z2)\n",
    "        \n",
    "        # Error = Output of the output layer - Training label\n",
    "        E = self.A2-Y\n",
    "        \n",
    "        # Mean Squared Error :\n",
    "        MSE = (1/2) * np.abs(np.power(E,2).sum())\n",
    "        self.cost.append(MSE)\n",
    "                \n",
    "    def evaluate(self,target_features):\n",
    "        \"\"\"\n",
    "        Comparing the target labels by neural network's\n",
    "        outputs, then returning accuracy score\n",
    "        \"\"\"\n",
    "        acc = (self.A2 == target_features).all().mean()\n",
    "        print(f\"Accuracy of the model is: {int(acc*100)}%\")\n",
    "        return int(acc*100)  \n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Feed forwarding from the input nodes, through the hidden nodes \n",
    "        and to the output nodes.\n",
    "        \"\"\"\n",
    "        return self.step(np.dot(self.step(np.dot(X,self.W1)),self.W2))  \n",
    "    \n",
    "    def display_results(self):\n",
    "        \"\"\"\n",
    "        Plotting and displaying the Mean Sqaured Error\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f\"MSE loss is : {self.cost[-1]}\")\n",
    "        plt.plot(range(len(np.squeeze(np.array(self.cost)))),np.squeeze(np.array(self.cost)))\n",
    "        plt.legend([f\"MSE:{round(self.cost[-1],4)}\"])\n",
    "        plt.xlabel('# of Iterations')\n",
    "        plt.ylabel('MSE Loss Function')\n",
    "        plt.title('Model evaluation')        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is: 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_partA = FeedForwardNN()\n",
    "model_partA.fit(input_features,target_output)\n",
    "model_partA.evaluate(target_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self):        \n",
    "         \n",
    "        # Introducing weight and bias terms,\n",
    "        # by standart normal distribution over 0\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Hidden Layers bias term :       \n",
    "        self.B1 = np.random.randn(4,1)\n",
    "        # Output Layers bias term :\n",
    "        self.B2 = np.array([[1]])\n",
    "        \n",
    "        # Learning rate for gradient descent :\n",
    "        self.lr = 0.05   \n",
    "                        \n",
    "        # Hidden Layers weight :\n",
    "        self.W1 = np.random.randn(4,4) * 0.01\n",
    "        # Output layers weight\n",
    "        self.W2 = np.random.randn(4,1) * 0.01\n",
    "        \n",
    "        # To track Mean Squared Error function :\n",
    "        self.cost = []\n",
    "        \n",
    "        # Concatenating weights and bias terms :\n",
    "        self.W1 = np.concatenate((self.W1,self.B1),axis = 1)\n",
    "        self.W2 = np.concatenate((self.W2,self.B2))\n",
    "        \n",
    "        # Output of the output layer\n",
    "        self.A2 = None\n",
    "        \n",
    "    def sigmoid(self,X:np.ndarray):\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    def sigmoid_der(self,X:np.ndarray):\n",
    "        \"\"\"\n",
    "        The derivative of Sigmoid function for\n",
    "        gradient descent while backproparagation\n",
    "        \"\"\"\n",
    "        return self.sigmoid(X)*(1-self.sigmoid(X))\n",
    "    \n",
    "    \n",
    "    def fit(self,X:np.ndarray,Y:np.ndarray,iterations:int,verbose:True):\n",
    "        \"\"\"\n",
    "        Given the traning dataset,their labels and number of epochs\n",
    "        fitting the model, and measure the performance\n",
    "        by validating training dataset.\n",
    "        \"\"\"                \n",
    "         \n",
    "        for epoch in range(iterations):\n",
    "            \n",
    "            # Feed forwarding :\n",
    "            Z1 = np.dot(X,self.W1) \n",
    "            \n",
    "            # Output of the hidden layer :\n",
    "            A1 = self.sigmoid(Z1)\n",
    "            \n",
    "            # Ä°nput of the output layer :\n",
    "            Z2 = np.dot(A1,self.W2) \n",
    "            # Output of the output layer :\n",
    "            self.A2 = self.sigmoid(Z2)\n",
    "            \n",
    "            # Calculating error\n",
    "            E = self.A2-Y\n",
    "            \n",
    "            # Mean sqaured error :\n",
    "            MSE = (1/2) * np.abs(np.power(E,2).sum())\n",
    "            self.cost.append(MSE)\n",
    "            \n",
    "            if verbose == True:\n",
    "                if epoch % 500 == 0:\n",
    "                    print(f\"The epoch num is: {epoch} ------> MSE is : {MSE}\")\n",
    "                    \n",
    "            # Back propagation part, updating weights and bias term\n",
    "            # according to the gradient descent algorithm, in the first phase\n",
    "            # I computed effect of the outputs layers weights on the loss function.\n",
    "            # Them, in the second phase, I computed the effect of the hidden layers\n",
    "            # weight on the Error function. Finally, update the weights and biases.\n",
    "            \n",
    "#----------------------- First Phase ------------------------------------#\n",
    "            # The derivative of the Error with respect to the output :\n",
    "            dE_dA2 = E \n",
    "            # The derivative of output with respect to input to the output layer :\n",
    "            dA2_dZ2 = self.sigmoid_der(Z2)\n",
    "            dE_dZ2 = E * dA2_dZ2\n",
    "            \n",
    "            # The derivative of the input function of output layer with respect to weights of\n",
    "            # the output layer :\n",
    "            dZ2_dW2 = A1.T\n",
    "            \n",
    "            # Total derivate :\n",
    "            dE_dW2 = np.dot(dZ2_dW2,dE_dZ2)\n",
    "            \n",
    "#----------------------- Second Phase ------------------------------------# \n",
    "            # Goal is to find the effect of the hidden layers weight \n",
    "            # on the output of neural network so we should find\n",
    "        \n",
    "            # The derivate of the Error function with respect to output of the hidden layer\n",
    "            dE_dA1 = np.dot(dE_dZ2,self.W2.T)\n",
    "            \n",
    "            # The derivative of the outputs of the hidden layer with respect to input to the \n",
    "            # hidden layer :\n",
    "            dA1_dZ1 = self.sigmoid_der(Z1)\n",
    "            \n",
    "            # Multiplication of above two :\n",
    "            dE_dZ1= dE_dA1 * dA1_dZ1\n",
    "            \n",
    "            # The derivative of the input of the hidden layer with respect to the weights of\n",
    "            # the hidden layer :\n",
    "            dZ1_dW1 = X.T\n",
    "            \n",
    "            # Total derivative:\n",
    "            dE_dW1 = np.dot(dZ1_dW1,dE_dZ1)\n",
    "            \n",
    "            # Updating weights and biases\n",
    "            self.W1 -= self.lr * dE_dW1\n",
    "            self.W2 -= self.lr * dE_dW2\n",
    "            \n",
    "                            \n",
    "    def accuracy(self,Y):\n",
    "        \"\"\"\n",
    "        Given the test labels of the training test, comparing and\n",
    "        returning the accuracy of the model\n",
    "        \"\"\"\n",
    "        acc = (self.A2 == target_features).all().mean()\n",
    "        print(f\"Accuracy of the model is: {int(acc*100)}%\")\n",
    "        return int(acc*100)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Feed forwarding from the input nodes, through the hidden nodes \n",
    "        and to the output nodes.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(np.dot(self.sigmoid(np.dot(X,self.W1)),self.W2))  \n",
    "    \n",
    "    def evaluate(self):  \n",
    "        \"\"\"Plotting and displaying the Mean Sqaured Error :        \n",
    "        \"\"\"\n",
    "        print(f\"MSE loss is : {self.cost[-1]}\")\n",
    "        plt.plot(range(len(np.squeeze(np.array(self.cost)))),np.squeeze(np.array(self.cost)))\n",
    "        plt.legend([f\"MSE:{round(self.cost[-1],4)}\"])\n",
    "        plt.xlabel('# of Iterations')\n",
    "        plt.ylabel('MSE Loss Function')\n",
    "        plt.title('Model evaluation') \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The epoch num is: 0 ------> MSE is : 0.9619788868874422\n",
      "The epoch num is: 500 ------> MSE is : 0.40325494024751635\n",
      "The epoch num is: 1000 ------> MSE is : 0.3795212467800225\n",
      "The epoch num is: 1500 ------> MSE is : 0.35341330636682233\n",
      "The epoch num is: 2000 ------> MSE is : 0.3132202597072006\n",
      "The epoch num is: 2500 ------> MSE is : 0.22804025638377756\n",
      "The epoch num is: 3000 ------> MSE is : 0.12364572046893026\n",
      "The epoch num is: 3500 ------> MSE is : 0.06851007874362827\n",
      "The epoch num is: 4000 ------> MSE is : 0.04303458148591599\n",
      "The epoch num is: 4500 ------> MSE is : 0.02982426525555422\n",
      "The epoch num is: 5000 ------> MSE is : 0.022157507630476345\n",
      "The epoch num is: 5500 ------> MSE is : 0.017304044604442875\n",
      "The epoch num is: 6000 ------> MSE is : 0.014021648979712172\n",
      "The epoch num is: 6500 ------> MSE is : 0.01168555922122016\n",
      "The epoch num is: 7000 ------> MSE is : 0.009954768413604405\n",
      "The epoch num is: 7500 ------> MSE is : 0.008630360355893677\n",
      "The epoch num is: 8000 ------> MSE is : 0.00758981041000524\n",
      "The epoch num is: 8500 ------> MSE is : 0.006754161786226416\n",
      "The epoch num is: 9000 ------> MSE is : 0.006070567000719698\n",
      "The epoch num is: 9500 ------> MSE is : 0.005502496156775224\n",
      "The epoch num is: 10000 ------> MSE is : 0.005023990645568032\n",
      "The epoch num is: 10500 ------> MSE is : 0.004616156571682226\n",
      "The epoch num is: 11000 ------> MSE is : 0.004264950995831126\n",
      "The epoch num is: 11500 ------> MSE is : 0.00395974212121355\n",
      "The epoch num is: 12000 ------> MSE is : 0.003692347909741339\n",
      "The epoch num is: 12500 ------> MSE is : 0.003456378983124807\n",
      "The epoch num is: 13000 ------> MSE is : 0.0032467800236260043\n",
      "The epoch num is: 13500 ------> MSE is : 0.0030595036459289755\n",
      "The epoch num is: 14000 ------> MSE is : 0.002891274507601646\n",
      "The epoch num is: 14500 ------> MSE is : 0.0027394160426920263\n",
      "The epoch num is: 15000 ------> MSE is : 0.0026017213952253967\n",
      "The epoch num is: 15500 ------> MSE is : 0.0024763560350119407\n",
      "The epoch num is: 16000 ------> MSE is : 0.002361783407181838\n",
      "The epoch num is: 16500 ------> MSE is : 0.0022567075474218793\n",
      "The epoch num is: 17000 ------> MSE is : 0.002160028344690801\n",
      "The epoch num is: 17500 ------> MSE is : 0.0020708063378806524\n",
      "The epoch num is: 18000 ------> MSE is : 0.0019882347740602826\n",
      "The epoch num is: 18500 ------> MSE is : 0.0019116172510148988\n",
      "The epoch num is: 19000 ------> MSE is : 0.0018403496929382164\n",
      "The epoch num is: 19500 ------> MSE is : 0.0017739057167865887\n",
      "The epoch num is: 20000 ------> MSE is : 0.0017118246727504962\n",
      "The epoch num is: 20500 ------> MSE is : 0.001653701809354282\n",
      "The epoch num is: 21000 ------> MSE is : 0.0015991801383704162\n",
      "The epoch num is: 21500 ------> MSE is : 0.0015479436686039838\n",
      "The epoch num is: 22000 ------> MSE is : 0.0014997117488656344\n",
      "The epoch num is: 22500 ------> MSE is : 0.0014542343149773808\n",
      "The epoch num is: 23000 ------> MSE is : 0.0014112878776859378\n",
      "The epoch num is: 23500 ------> MSE is : 0.0013706721209835303\n",
      "The epoch num is: 24000 ------> MSE is : 0.0013322070058302414\n",
      "The epoch num is: 24500 ------> MSE is : 0.0012957302943198581\n",
      "The epoch num is: 25000 ------> MSE is : 0.0012610954251905792\n",
      "The epoch num is: 25500 ------> MSE is : 0.0012281696842001899\n",
      "The epoch num is: 26000 ------> MSE is : 0.0011968326229795446\n",
      "The epoch num is: 26500 ------> MSE is : 0.0011669746880950836\n",
      "The epoch num is: 27000 ------> MSE is : 0.0011384960286106081\n",
      "The epoch num is: 27500 ------> MSE is : 0.001111305455764613\n",
      "The epoch num is: 28000 ------> MSE is : 0.0010853195327236434\n",
      "The epoch num is: 28500 ------> MSE is : 0.0010604617759309412\n",
      "The epoch num is: 29000 ------> MSE is : 0.0010366619524971902\n",
      "The epoch num is: 29500 ------> MSE is : 0.0010138554604980665\n",
      "The epoch num is: 30000 ------> MSE is : 0.0009919827810478264\n",
      "The epoch num is: 30500 ------> MSE is : 0.0009709889926864963\n",
      "The epoch num is: 31000 ------> MSE is : 0.0009508233400112932\n",
      "The epoch num is: 31500 ------> MSE is : 0.0009314388496500897\n",
      "The epoch num is: 32000 ------> MSE is : 0.0009127919876565141\n",
      "The epoch num is: 32500 ------> MSE is : 0.0008948423532336596\n",
      "The epoch num is: 33000 ------> MSE is : 0.0008775524043939309\n",
      "The epoch num is: 33500 ------> MSE is : 0.0008608872117563787\n",
      "The epoch num is: 34000 ------> MSE is : 0.0008448142371887942\n",
      "The epoch num is: 34500 ------> MSE is : 0.0008293031344327856\n",
      "The epoch num is: 35000 ------> MSE is : 0.0008143255692194264\n",
      "The epoch num is: 35500 ------> MSE is : 0.0007998550566993827\n",
      "The epoch num is: 36000 ------> MSE is : 0.0007858668142836539\n",
      "The epoch num is: 36500 ------> MSE is : 0.00077233762822569\n",
      "The epoch num is: 37000 ------> MSE is : 0.0007592457324782079\n",
      "The epoch num is: 37500 ------> MSE is : 0.0007465706985336054\n",
      "The epoch num is: 38000 ------> MSE is : 0.0007342933351092174\n",
      "The epoch num is: 38500 ------> MSE is : 0.0007223955966708912\n",
      "The epoch num is: 39000 ------> MSE is : 0.0007108604999040691\n",
      "The epoch num is: 39500 ------> MSE is : 0.0006996720473421378\n",
      "The epoch num is: 40000 ------> MSE is : 0.0006888151574500179\n",
      "The epoch num is: 40500 ------> MSE is : 0.0006782756005383647\n",
      "The epoch num is: 41000 ------> MSE is : 0.0006680399399515503\n",
      "The epoch num is: 41500 ------> MSE is : 0.0006580954780323251\n",
      "The epoch num is: 42000 ------> MSE is : 0.0006484302064187239\n",
      "The epoch num is: 42500 ------> MSE is : 0.0006390327602751635\n",
      "The epoch num is: 43000 ------> MSE is : 0.00062989237610096\n",
      "The epoch num is: 43500 ------> MSE is : 0.0006209988527955857\n",
      "The epoch num is: 44000 ------> MSE is : 0.0006123425156926255\n",
      "The epoch num is: 44500 ------> MSE is : 0.0006039141833026682\n",
      "The epoch num is: 45000 ------> MSE is : 0.0005957051365313234\n",
      "The epoch num is: 45500 ------> MSE is : 0.0005877070901608983\n",
      "The epoch num is: 46000 ------> MSE is : 0.0005799121664048471\n",
      "The epoch num is: 46500 ------> MSE is : 0.0005723128703620848\n",
      "The epoch num is: 47000 ------> MSE is : 0.0005649020672145237\n",
      "The epoch num is: 47500 ------> MSE is : 0.0005576729610257127\n",
      "The epoch num is: 48000 ------> MSE is : 0.0005506190750115651\n",
      "The epoch num is: 48500 ------> MSE is : 0.0005437342331657141\n",
      "The epoch num is: 49000 ------> MSE is : 0.0005370125431328357\n",
      "The epoch num is: 49500 ------> MSE is : 0.0005304483802324302\n"
     ]
    }
   ],
   "source": [
    "model_partC = MultiLayerPerceptron()\n",
    "model_partC.fit(X = input_features,Y = target_output,iterations = 50000,verbose = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss is : 0.0005240490481591191\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "model_partC.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Robust Weights of Hidden Layer \n",
      " [[-1.26240984 -1.46796279 -1.43157073 -1.60343491  2.90054917]\n",
      " [-1.26238999 -1.46804527 -1.44813112 -1.61921153  2.85520524]\n",
      " [-1.2505314  -1.46779474 -1.44178554 -1.5993274   2.9085472 ]\n",
      " [-1.24683938 -1.43399215 -1.41755183 -1.57873098  3.03688246]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most Robust Weights of Hidden Layer \\n {model_partC.W1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Robust Weights of Output Layer \n",
      " [[-3.32662012]\n",
      " [-3.86788154]\n",
      " [-3.79990092]\n",
      " [-4.26749739]\n",
      " [ 7.80405911]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most Robust Weights of Output Layer \\n {model_partC.W2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 400 x 4 zero matrix :\n",
    "input = np.zeros(1600).reshape(400,4)\n",
    "\n",
    "# Generating 400 input sample = 16 x 25\n",
    "for i in range(16):\n",
    "    for j in range(25):\n",
    "        input[i*25+j,:] = input_features[i]\n",
    "\n",
    "# Noice matrix with Gaussion distribution zero mean 0.2 std.\n",
    "noise = np.random.normal(loc = 0 , scale = 0.2 ,size = (400,4))\n",
    "\n",
    "# Concetaneting\n",
    "noisy_samples = input + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The shape of noisy samples: (400, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\" The shape of noisy samples: {noisy_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the model created in the Part A :\n",
    "model_A_pred = model_partA.predict(noisy_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The predictions on the model created in the Part C :\n",
    "model_C_pred = model_partC.predict(noisy_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating testing labels :\n",
    "test_labels = np.repeat(1,400).reshape(400,1)\n",
    "test_labels[:25] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the predictions of the model created\n",
    "# in the Part C since sigmoid function returns the\n",
    "# predictions in the interval [0,1]. \n",
    "model_C_pred = np.round(model_C_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X1,X2):\n",
    "    \"\"\"\n",
    "    Given two arrays, prediction matrix and\n",
    "    test matrix, comparing and returning the number\n",
    "    of true predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(X1.shape == X2.shape) \n",
    "    \n",
    "    true_pred = 0\n",
    "    \n",
    "    for i in range(X1.shape[0]):\n",
    "        if int(X1[i]) == 1*int(X2[i]):\n",
    "            true_pred += 1\n",
    "    return true_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_perf = performance(model_A_pred,test_labels)\n",
    "model_C_perf = performance(model_C_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Model Part A is: 94.0% \n",
      "\n",
      "The Accuracy of Model Part C is: 97.75% \n"
     ]
    }
   ],
   "source": [
    "print(f\"The Accuracy of Model Part A is: {(model_A_perf/test_labels.shape[0]) * 100}% \\n\")\n",
    "print(f\"The Accuracy of Model Part C is: {(model_C_perf/test_labels.shape[0]) * 100}% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
